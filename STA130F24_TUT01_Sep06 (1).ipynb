{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da914c6",
   "metadata": {},
   "source": [
    "# STA130 TUT 01 (Sep06)<br><br> üèÉüèª‚Äç‚ôÄÔ∏è üèÉüèª <u> Hitting the ground running... <u><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8969b8",
   "metadata": {},
   "source": [
    "\n",
    "## üöß üèóÔ∏è (Using notebooks and ChatBots) Demo [45 minutes]  \n",
    "      \n",
    "#### 1. *[About 8 of the 45 minutes]* Demonstrate going to the course [Quercus homepage](https://q.utoronto.ca/courses/354091); accessing the [Course GitHub Repo](https://github.com/pointOfive/STA130_ChatGPT); opening new and uploaded notebooks on [UofT Jupyterhub](https://datatools.utoronto.ca) (classic jupyter notebook, or jupyterhub is fine, or students may use [google collab](https://colab.research.google.com/)); and using Jupyter notebooks as a \"`Python` calculator\" and editing [\"Markdown cells\"](https://www.markdownguide.org/cheat-sheet/)<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> This is all simple and pretty obvious intended to be that way so students get that they can do this on their own.\n",
    "\n",
    "</details>\n",
    "    \n",
    "#### 2. *[About 30 of the 45 minutes]* Demonstrate using [ChatGPT](https://chat.openai.com/) (or [Copilot](https://copilot.microsoft.com/) if conversation privacy is desired) to\n",
    "\n",
    "1. find an (a) \"amusing, funny, or otherwise interesting dataset\" which (b) has missing values and is (c) available online through a URL link to a csv file;\n",
    "2. load the data into the notebook with `pandas` and get \"missing data counts\" for the dataset;\n",
    "3. prompt the ChatBot to \"Please provide a summary of our interaction for submitting as part of the requirements of an assignment\"; and, to \"Please provide me with the final working verson of the code that we created\"<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> The intention here is to demonstrate that [at the current GPT4.0-ish level] the ChatBot (with probably about an 80% chance) **cannot fullfil all the requests of the inquiry (of (a) \"funny or amusing\" nature, (b) the presence of missingness, and (c) working url links)** *but will otherwise produce working code*<br><br>\n",
    "> \n",
    "> 1. ChatBots have a notoriously \"short term memory\"; so, be ready for them to \"forget\" specific details of your prompting requests \n",
    "> 2. ChatBots often cannot pivot away substantially from initial answers; so, be ready for your efforts at follow up and correction with the ChatBot to prove frustratingly futile (which, may in this case actually have a lot to do with the following fact, that...)\n",
    "> 3. ChatBots don't seem to be very aware of the contents of datasets that are avalable online (or even working url links where datasets are); so, ChatBot are not currently a substitue for exploring dataset repository such as [TidyTuesday](https://github.com/rfordatascience/tidytuesday) (or other data repositiory resources) and reviewing data yourself (although, ChatBot interactions can nonetheless be help with brainstorm dataset ideas and provide a way to \"search for content\", perhaps especially when referencing a specific website in the conversation)<br><br>\n",
    "> \n",
    "> Examples of this task going pretty well are available [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00006_copilot_funnyamusingNAdatasetV3.md), [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00007_copilot_funnyamusingNAdatasetV4.md), and [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00001_gpt3p5_villagersdata.md); while, examples of this going poorly are available [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00002_copilot_funnyamusingNAdataset.md) and [here](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00002_gpt3p5_funnyasusingNAdataset.md). Successes and failures are found within the Microsoft Copilot and ChatGPT ChatBots both, suggesting the quality of the results likely has to do more with \"randomness\" and perhaps the nature of the prompting and engagement as opposed to the actual ChatBot version being used...\n",
    "    \n",
    "</details>\n",
    "\n",
    "#### 3. *[About 7 of the 45 minutes]* Demonstrate saving your python jupyter notebook in your own account and \"repo\" on [github.com](https://github.com), and sharing (a) notebook links, (b), ChatBot transcript log links, (c) ChatBot summaries through a piazza post and a Quercus announcement (so students can use this later for their homework assignment if they wish)<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379538b1",
   "metadata": {},
   "source": [
    "## üí¨ üó£Ô∏è Communication [55 minutes]  \n",
    "     \n",
    "#### 1. *[About 15 of the 55 minutes]* Ice breakers  and introductions, in 8 groups of 3 or thereabouts...\n",
    "    \n",
    "1. Each person may bring two emojis to a desert island... reveal your emojis at the same time... for emojis selected more than once the group should select one additional emoji\n",
    "2. Where are you from, what do you think your major might be, and what's an \"interesting\" fact that you're willing to share about yourself?\n",
    "        \n",
    "#### 2. *[About 10 of the 55 minutes]* These are where all the bullet holes were in the planes that returned home so far after some missions in World War II\n",
    "    \n",
    "1. Where would you like to add armour to planes for future missions?\n",
    "2. Hint: there is a hypothetical dataset of the bullet holes on the planes that didn't return which is what we'd ideally compare against the dataset we observe...\n",
    "        \n",
    "![Classic image of survivorship bias of WW2 planes](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Survivorship-bias.svg/640px-Survivorship-bias.svg.png)\n",
    "           \n",
    "#### 3. *[About 10 of the 55 minutes]* Monte Hall problem: there is a gameshow with three doors, one of which has a prize, and you select one of the doors and the gameshow host reveals one of the other two unchosen doors which does not have the prize... would you like to change your guess to the other unchosen door?\n",
    "\n",
    "![](https://mathematicalmysteries.org/wp-content/uploads/2021/12/04615-0sxvwbnzvvnhuklug.png)<br>\n",
    "       \n",
    "#### 4. *[About 20 of the 60 minutes]* Discuss the experience of the groups for the WW2 planes and Monte Hall problems\n",
    "\n",
    "1. For each of these problems, have students vote on whether their groups (a) agreed on answers from the beginning, (b) agreed only after some discussion and convincing, or (c) retained somewhat divided in their opinions of the best way to proceed<br><br>\n",
    "    \n",
    "2. Briefely identify the correct answer from the answers the groups arrived at<br><br>\n",
    "    \n",
    "3. **[If time permits... otherwise this is something students could consider after TUT]** Prompt a [ChatGPT](https://chat.openai.com/) [or [Copilot](https://copilot.microsoft.com/)] ChatBot to introduce and explain \"survivorship bias\" using spotify songs as an example and see if students are able to generalize this idea for the WW2 planes problem and if they find it to be a convincing argument to understand the problem<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> This could be done like [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00009_copilot_survivorshipbias_spotify.md) or [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00003_gpt3p5_spotify_Survivorship_Bias.md), or you could instead try to approach things more generally like [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00004_gpt3p5_general_Survivorship_Bias.md)\n",
    "> \n",
    "> Two ends of the ChatBot prompting spectrum are\n",
    "> \n",
    "> 1. creating an extensive prompt exhuastively specifying the desired response results; or, \n",
    "> 2. iteratively clarifying the desired response results through interactive ChatBot dialogue<br><br>\n",
    "> \n",
    "> This is to some degree a matter of preference regarding the nature of ChatBot conversation sessions, but there it may also be a lever to influence the nature of the responses provided by the ChatBot \n",
    "</details>\n",
    "    \n",
    "4. **[If time permits... otherwise this is something students could consider after TUT]** Prompt a [ChatGPT](https://chat.openai.com/) [or [Copilot](https://copilot.microsoft.com/)] ChatBot to introduce and explain the Monte Hall problem and see if the students find it understandable and convincing<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> ChatBots fail to correctly analyze the Monte Hall problem when they're asked for a formal probabilistic argument...\n",
    ">\n",
    "> - [ChatGPT fails by wrongly calculating a probability of 1/2...](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00005_gpt3p5_MonteHallWrong.md)\n",
    "> - [Copilot fares similarly poorly without substantial guidance...](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/COP/SLS/00010_copilot_montehallwrong.md)<br><br>\n",
    "> \n",
    "> *demonstrating (a) that there are clear limits to how deeply ChatBots actually \"reason\", and (b) that they are instead better understood as simply being information regurgitation machines, and (c) that this means  they can suffer from the \"garbage in, garbage out\" problem if the quality of the information their responses are based on are is poor and inaccurate (as is notoriously the case in the Monte Hall problem, for which many incorrect mathematical analyses have been \"published\" into the collection of human generated textual data on which ChatBots are based)*\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03fb0d",
   "metadata": {},
   "source": [
    "# Funny Data Sets Help\n",
    "\n",
    "#### You:\n",
    "Hey chat I want to find a really funny, lighthearted and entertaining data set, can you help me with providing some links to the such datasets?\n",
    "\n",
    "#### ChatGPT:\n",
    "Sure, finding funny and lighthearted datasets can be a fun endeavor! Here are a few suggestions:\n",
    "\n",
    "1. **Reddit Jokes Dataset**: This dataset contains jokes from Reddit, categorized by humor type and ratings. You can find it [here](https://github.com/taivop/joke-dataset).\n",
    "\n",
    "2. **Twitter Sentiment Analysis**: While not exclusively funny, analyzing tweets with sentiment analysis can uncover humorous tweets and trends. Check out datasets from platforms like [Kaggle](https://www.kaggle.com/datasets) or academic repositories.\n",
    "\n",
    "3. **Internet Memes Dataset**: Memes are often humorous and reflect current cultural trends. Look for meme datasets on platforms like [Know Your Meme](https://knowyourmeme.com/) or [Meme Scraper](https://github.com/R3abZ/Meme-Scraper).\n",
    "\n",
    "4. **Funny Amazon Reviews**: Some websites compile hilarious product reviews from Amazon. You can find datasets with scraped humorous reviews online.\n",
    "\n",
    "5. **Stand-up Comedy Transcripts**: Websites like [Kaggle](https://www.kaggle.com/) often host datasets of stand-up comedy transcripts, which can be quite entertaining and humorous.\n",
    "\n",
    "These datasets should provide a good mix of lighthearted and entertaining content for your analysis or enjoyment!\n",
    "\n",
    "\n",
    "#### You:\n",
    "Hey only the last link works, can you give me urls that actually work\n",
    "\n",
    "#### ChatGPT:\n",
    "Certainly! Here are some updated and verified links to datasets that you might find amusing:\n",
    "\n",
    "1. **Reddit Jokes Dataset**: Contains jokes from Reddit, categorized by type and ratings.\n",
    "   - [Reddit Jokes Dataset](https://github.com/taivop/joke-dataset)\n",
    "\n",
    "2. **Twitter Sentiment Analysis Datasets**: While not exclusively funny, they can include humorous tweets.\n",
    "   - [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "   - [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "\n",
    "3. **Internet Memes Dataset**:\n",
    "   - [Know Your Meme](https://knowyourmeme.com/) (for meme trends and culture)\n",
    "   - [Meme Scraper on GitHub](https://github.com/R3abZ/Meme-Scraper)\n",
    "\n",
    "4. **Funny Amazon Reviews**: Look for datasets with scraped humorous reviews online, though these might require more specific searches or scraping.\n",
    "\n",
    "5. **Stand-up Comedy Transcripts**: Often found on platforms like Kaggle, focusing on transcripts of stand-up routines.\n",
    "   - [Kaggle Stand-up Comedy Datasets](https://www.kaggle.com/)\n",
    "\n",
    "These links should lead you to datasets that are either directly humorous or provide content that can be analyzed for humor and lightheartedness.\n",
    "\n",
    "\n",
    "#### You:\n",
    "I like the reddit Jokes dataset, does it have any missing values in it? I want a dataset with missing data so that I can practice my datawrangling\n",
    "\n",
    "#### ChatGPT:\n",
    "The Reddit Jokes dataset might not explicitly mention missing values in its documentation, but handling missing data is a common challenge in data wrangling. If you're looking specifically for datasets with missing values to practice your data wrangling skills, here are a few suggestions:\n",
    "\n",
    "1. **Titanic Dataset**: A classic dataset often used for machine learning exercises, available on platforms like [Kaggle](https://www.kaggle.com/c/titanic/data). It includes missing values that need to be handled.\n",
    "\n",
    "2. **UCI Machine Learning Repository**: Offers various datasets where some may contain missing values. Explore datasets like the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/adult) or others.\n",
    "\n",
    "3. **Airbnb Listings Dataset**: Some versions of Airbnb listings datasets may contain missing values in various columns like reviews or amenities.\n",
    "\n",
    "4. **Medical Datasets**: Datasets related to health or medical fields often contain missing data due to incomplete records or data entry errors.\n",
    "\n",
    "5. **Survey Datasets**: Surveys often have missing responses, making them good candidates for practicing data imputation and cleaning.\n",
    "\n",
    "For a specific dataset like the Reddit Jokes dataset, you might need to inspect it yourself or look at user comments or documentation on GitHub to see if missing values are explicitly mentioned. Generally, any real-world dataset can be expected to have missing values that need to be addressed during analysis.\n",
    "\n",
    "\n",
    "#### You:\n",
    "I want a dataset that has both elements, I want it to be funny and I want it to have missing data\n",
    "\n",
    "#### ChatGPT:\n",
    "Finding a dataset that combines humor and missing data can be a bit challenging, as humorous datasets often focus less on structured data integrity. However, you can create an interesting exercise by combining a dataset known for humor with artificially introduced missing values. Here‚Äôs a creative approach:\n",
    "\n",
    "1. **Reddit Jokes Dataset**: Use the Reddit Jokes dataset you liked earlier for its humorous content.\n",
    "\n",
    "2. **Artificially Introduce Missing Values**: Take the Reddit Jokes dataset and intentionally introduce missing values into some columns. This can simulate real-world scenarios where data might be incomplete or improperly recorded.\n",
    "\n",
    "3. **Practice Data Wrangling**: Use tools like Python's pandas library or R to practice handling missing data. Techniques such as imputation, dropping rows or columns with missing values, and exploring patterns in missing data can all be applied.\n",
    "\n",
    "By combining a humorous dataset with intentionally introduced missing data, you can create an engaging learning experience that hones your data wrangling skills while enjoying the humorous content of the jokes.\n",
    "\n",
    "\n",
    "#### You:\n",
    "Thats a great idea, can you please write the code to import this jokes dataset and introduce some missing data artificually, Im using python and pandas\n",
    "\n",
    "#### ChatGPT:\n",
    "Certainly! Here‚Äôs a step-by-step guide to importing the Reddit Jokes dataset using Python and pandas, and then introducing some artificial missing data:\n",
    "\n",
    "### Step 1: Import Required Libraries\n",
    "\n",
    "First, make sure you have pandas installed. If not, you can install it using `pip install pandas`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "### Step 2: Importing the Reddit Jokes Dataset\n",
    "\n",
    "Assuming you've downloaded the Reddit Jokes dataset (available [here](https://github.com/taivop/joke-dataset)), you can load it into a pandas DataFrame:\n",
    "\n",
    "```python\n",
    "# Replace 'path_to_your_dataset' with the actual path to your downloaded dataset file\n",
    "jokes_df = pd.read_csv('path_to_your_dataset')\n",
    "```\n",
    "\n",
    "### Step 3: Introducing Artificial Missing Data\n",
    "\n",
    "Now, let's introduce some artificial missing data into the DataFrame:\n",
    "\n",
    "```python\n",
    "# Introduce missing data by randomly selecting rows and columns\n",
    "# Here we're introducing missing data in 10% of the rows and 20% of the columns\n",
    "rows_to_modify = jokes_df.sample(frac=0.1).index\n",
    "cols_to_modify = jokes_df.sample(frac=0.2, axis=1).columns\n",
    "\n",
    "jokes_df.loc[rows_to_modify, cols_to_modify] = np.nan\n",
    "```\n",
    "\n",
    "### Step 4: Handling Missing Data (Optional)\n",
    "\n",
    "You can practice handling missing data using pandas methods like `isnull()`, `fillna()`, or `dropna()`:\n",
    "\n",
    "```python\n",
    "# Example of dropping rows with any missing values\n",
    "jokes_df_cleaned = jokes_df.dropna()\n",
    "```\n",
    "\n",
    "### Step 5: Explore and Analyze\n",
    "\n",
    "Finally, explore the dataset to analyze the impact of missing data on your analysis or processing pipeline.\n",
    "\n",
    "### Example Summary\n",
    "\n",
    "Here‚Äôs how it all fits together:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Reddit Jokes dataset\n",
    "jokes_df = pd.read_csv('path_to_your_dataset')\n",
    "\n",
    "# Introduce artificial missing data\n",
    "rows_to_modify = jokes_df.sample(frac=0.1).index\n",
    "cols_to_modify = jokes_df.sample(frac=0.2, axis=1).columns\n",
    "jokes_df.loc[rows_to_modify, cols_to_modify] = np.nan\n",
    "\n",
    "# Example: handling missing data\n",
    "jokes_df_cleaned = jokes_df.dropna()\n",
    "\n",
    "# Explore and analyze the dataset\n",
    "print(jokes_df_cleaned.head())\n",
    "```\n",
    "\n",
    "Replace `'path_to_your_dataset'` with the actual path to your downloaded Reddit Jokes dataset file. This script will load the dataset, introduce missing values, and demonstrate a basic method (`dropna()`) for handling missing data. You can expand on this with more complex data wrangling techniques as needed.\n",
    "\n",
    "\n",
    "#### You:\n",
    "I dont want to download the file, I want to use the same code but use a link\n",
    "\n",
    "#### ChatGPT:\n",
    "If you prefer to directly use a link to the Reddit Jokes dataset without downloading the file locally, you can modify the code to load the dataset directly from its URL. Here‚Äôs how you can do it using pandas:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# URL of the Reddit Jokes dataset\n",
    "url = 'https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json'\n",
    "\n",
    "# Load the dataset from the URL\n",
    "jokes_df = pd.read_json(url)\n",
    "\n",
    "# Introduce artificial missing data\n",
    "rows_to_modify = jokes_df.sample(frac=0.1).index\n",
    "cols_to_modify = jokes_df.sample(frac=0.2, axis=1).columns\n",
    "jokes_df.loc[rows_to_modify, cols_to_modify] = np.nan\n",
    "\n",
    "# Example: handling missing data\n",
    "jokes_df_cleaned = jokes_df.dropna()\n",
    "\n",
    "# Explore and analyze the dataset\n",
    "print(jokes_df_cleaned.head())\n",
    "```\n",
    "\n",
    "In this script:\n",
    "\n",
    "- `pd.read_json(url)` loads the JSON data directly from the provided URL into a pandas DataFrame.\n",
    "- Artificial missing data is introduced similarly as before, by randomly selecting rows and columns and setting their values to `NaN`.\n",
    "- The rest of the script remains the same for handling and exploring the dataset.\n",
    "\n",
    "Replace `url` with the actual URL of the Reddit Jokes dataset (`'https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json'` in this case). This approach allows you to work with the dataset without downloading it locally, directly from its online source."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
